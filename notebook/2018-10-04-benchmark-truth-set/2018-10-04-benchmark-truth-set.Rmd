---
layout: default
#title: 
#featimg: 
---

Our benchmark experiment uses paired samples of DNA mixes from the four CEPH/Utah grandparents to model somatic variants.  We refer to such sample pairs as benchmark sample pairs and aim to label as positive those variants therein, which are clearly somatic.  Labeling is done based on gold standard germline variant call sets from the unmixed grandparental DNA, and results in a truth set for each benchmark sample pair.  The truth set contains somatic variants (the positives) distributed into 14 distinct alternative allele frequencies (AAF).  We began our study with benchmark sample pair Mix1A:Mix3A but realized the following shortcomings of the initial design: (1) poor decisions on labeling different types of variants; (2) too many positives; (3) the dependence on AAF could be determined only for recall but not for precision.  Here we revise our design.

## Terms

* *real experiment*: a design using a NeuN+ *case sample* and muscle *control sample*
  * I suggest the name *control sample* instead of *reference sample* because "reference" already qualifies the GRCh37decoy reference genome
* *benchmark experiment*: designed to model a real experiment under known somatic variants and other controlled conditions
* *initial benchmark design* (see below) was used for the analysis presented by Chaggai on the BSMN call on 10/4/2018
* *individual sample*: genomic DNA of one of the four CEPH/Utah grandparents (NA12889, NA12890, NA12891, NA12892)
* *(benchmark) mix*: a mixture $$m \in \{\mathrm{Mix}1, \mathrm{Mix}2, \mathrm{Mix}3 \}$$ of individual samples NA12889, NA12890, NA12891, NA12892 in 3 different proportions:
  * Mix1 4:8:16:72
  * Mix2 2:4:8:86
  * Mix3 0:0:0:100
* *mix replicate*: each mix was aliquoted and sequenced in three replicates A, B, C
* *benchmark sample pair*: a pair of possibly identical mix replicates; the initial design used Mix1A:Mix3A as case and control sample, respectively
* *callset*: records in a *filtered* or *unfiltered* VCF file
* *benchmark somatic callset* is generated by---typically---a somatic caller based on a benchmark sample pair
* *benchmark germline callset*: filtered callset generated by a germline caller (strelka or strelka2) based on an individual sample
  * *Illumina germline callset* were downloaded from [Illumina's BaseSpace](https://basespace.illumina.com/home/index)
  * *Chesslab germline callset* were generated in our lab from our sequencing data
* *benchmark variant*: a variant in any of the benchmark germline callsets
* *truth table*: constructed by Chaggai from benchmark variants of all four pairs of benchmark germline callsets (each callset pair is Illumina/Chesslab)
  * only snps and mnps (single and multiple nucleotide polymorphisms)
  * rows: benchmark variants
  * columns: individual genotypes as well as alternative nucleotide allele frequencies, AAF, for each nucleotide and benchmark sample pair
* *benchmark variant type*: based on the truth table and a given benchmark sample pair; for the initial design (Mix1:Mix3 sample pair) four types were defined:
  1. *conflicting genotype* for a given individual there is inconsistency in genotype between the Illumina and Chesslab germline callset
  1. *100% alternative allele* all individuals have the same AA genotype, while the reference genome (GRCh37) has RR, where A: alternative allele, R: reference allele
  1. *heterozygous control sample* the control sample (Mix3 = NA12892) has RA genotype
  1. *homozygous control sample* the control sample (Mix3 = NA12892) has RR or AA genotype
* *labeling*: assigning either *positive* or *negative* label to (candidate) variants in a given benchmark sample pair
  * in the present context positive labels somatic variants and negative anything but those note that both positives and negatives may be germline variants;
  * in the truth table benchmark variants are labeled by considering which benchmark variant types represent somatic variants; the positively labeled variants are the *(actual) positives* in the sense of ground truth
  * in the context of a benchmark somatic callset labeling is nothing but calling and filtering; these *called positives* or simply *calls* are either *true positives* or *false positives*; the (actual) positives not called are *false negatives*
* let $$\mathrm{E} [N_\mathrm{TP}], \mathrm{E} [N_\mathrm{FP}], \mathrm{E} [N_\mathrm{FN}]$$ denote the expected number of true positives, false positives and false negatives
    * *precision* is defined as $$\mathrm{E} [N_\mathrm{TP}] / (\mathrm{E} [N_\mathrm{TP}] + \mathrm{E} [N_\mathrm{FP}])$$
    * *recall* is defined as $$\mathrm{E} [N_\mathrm{TP}] / (\mathrm{E} [N_\mathrm{TP}] + \mathrm{E} [N_\mathrm{FN}])$$
* *precision-recall curve* is the set of precision and recall values evaluated at an ordered sequence of filter settings; such an ordered sequence is given by the probability that a call is true, estimated by VariantMetaCaller

## Initial design

|-------------------------------------------------------------------------------
|variant type in benchmark sample pair |label (is somatic variant?)|filter out?|
|-------------------------------------------------------------------------------
|conflicting genotype                  |    negative        |    no            |
|100% alternative allele               |    negative        |    no            |
|heterozygous control sample           |    positive        |    no            |
|homozygous control sample             |    positive        |    no            |
|-------------------------------------------------------------------------------

## New design

|-------------------------------------------------------------------------------
|variant type in benchmark sample pair |label (is somatic variant?)|filter out?|
|-------------------------------------------------------------------------------
|conflicting genotype                  |    negative        |    yes           |
|100% alternative allele               |    negative        |    no            |
|heterozygous control sample           |    negative        |    no            |
|homozygous control sample             |    positive        |remove some       |
|-------------------------------------------------------------------------------

### Notes

* The *filter out?* column in the tables above means that benchmark variants of a certain type should (or should not) be filtered out from any benchmark somatic callset.
* The *conflicting genotype* subset of benchmark variants will be filtered out because they emerge from differences between the germline calling workflows of Illumina and Chesslab therefore they reflect technical noise.
* The *remove some* for homozygous control sample means random filtering in order to diminish the excessive number of positives to a biologically realistic number (see discussion below)

### What's changed

Observe the changes. The new design

1. removes all benchmark variants with heterozygous control sample from the set of positives
1. filters out benchmark variants with conflicting genotype
1. removes some of the benchmark variants with homozygous control sample (see section *Remaining positives* below)

## Remaining positives (homozygous control sample)

When designing the number of positives, i.e. the how many "homozygous" benchmark variants should remain after filtering, we must consider what is or might be biologically realistic.

### Biologically realistic number of somatic variants

We can regard the remaining (modeled) clearly somatic variants as the set of actual positives but this set is orders of magnitude larger than what we expect, which leads to highly inflated precision.  Therefore we discard also most of these somatic variants to match the size of the positive set in our benchmark to that expected in real tissue pairs.

Chr22 alone contains 42289 of benchmark variants with homozygous control sample.  On the other hand, single cell sequencing studies estimated, for the whole genome, 2000 or even just 100 somatic mutations per cell in average; see [Lodato 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4664477/) and [Hazen 2016](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4795965/), respectively.  Moreover, it was suggested by both studies that the majority of these mutations are not recurrent in (or shared by) cells.  Thus, there seem to be orders of magnitude fewer somatic variants per genome per bulk tissue sample (pair) than positives in the initial design.  This holds especially for somatic variants with higher AAF.

### Two cases for the number of remaining positives

The *remove some* in the new design alludes to the plan to randomly filter the positives (homozygous control sample) to approach the hypothetical biological reality that the number of somatic variants in the genome is orders of magnitude lower than the number of positives in the initial design.  There are two random filters (below), each with different rationale.

|---------------------------------------------------------------------------------------
| case | remaining positives | genomic region | AAF spectrum  | rationale |
|---------------------------------------------------------------------------------------
| 1 | 100 | whole genome |  uniform |  biologically realistic |
| 2 | 1000 | chr22 |  fixed at a single frequency $$f$$ |  accurate precision-recall estimation for any given $$\mathrm{AAF} = f$$ |
|-------------------------------------------------------------------------------

### A priori and posteriori filtering

Filtering can be done *a priori* and *a posteriori* with respect to running VariantMetaCaller.

* *a priori*: filter the input VCF files to VariantMetaCaller
* *a posteriori*: filter the output VCF file of VariantMetaCaller

This difference affects the results since in the *a priori* case the removed variants are not part of the training set of VariantMetaCaller while in the *a posteriori* case they are.  This is especially important for case 2 of filtering (above). The top and bottom expressions below clarify the conditions for obtaining precision-recall relationships for the *a priori* and *a posteriori* method, respectively:

$$
\begin{eqnarray}
\mathrm{precision-recall} \, &|& \, \text{training spectrum} = \{f\},  \mathrm{AAF} = f \\
\mathrm{precision-recall} \, &|& \, \text{uniform training spectrum},  \mathrm{AAF} = f
\end{eqnarray}
$$

### Dependence of precision on alternative allele frequency

Conditioning recall and precision on AAF requires different kind of information.

1. recall: AAF for each modeled somatic variant; available
1. precision: AAF for each call; unavailable

The AAF for each call is unavailable in the initial design either because if the call is true the caller cannot provide a precise and unbiased estimate for AAF and if the call is false there is no variant thus AAF is undefined.  But the design of the benchmark experiment can be modified to fix that.  One needs to remove all variants but those whose AAF equals to a certain value.

## Design

Suppose $$\mathrm{E} N_\mathrm{paired} = 1000$$ is realistic for the whole genome.  The expected number of positives in the unrealistic initial design:

```{r}
len <- list(genome = 3153681224, chr22 = 51304566)
npos <- list(genome = NA, chr22 = 42289)
(npos$genome <- npos$chr22 * len$genome / len$chr22)
```

So we have `r npos$genome` positives instead of only 1000 positives, which means we need to retain only a fraction of `r 1000 / npos$genome` of them.  Let's round it to $$4 \times 10^{-4}$$!

It may be worth trying also $$\mathrm{E} N_\mathrm{paired} = 10000$$ and 100000, which correspond to retaining fractions of $$4 \times 10^{-3}$$ and $$4 \times 10^{-2}$$, respectively.

1. let $$\mathrm{AAF} = f$$; note that $$f\in E$$, the set of possible AAFs given the DNA mix
1. remove variants for which NA12892/Mix3 is heterozygous
1. remove variants for which our variant calling results contradict Illumina's results
1. remove purely germline variants (the *100% alternative allele variants* in Chaggai's jargon)
1. (optional) remove all variants but those whose AAF equals to $$f$$
1. using the remaining variants randomly retain a fraction of $$4 \times 10^{-2}$$, $$4 \times 10^{-3}$$ and $$4 \times 10^{-4}$$ as actual positives and filter out the rest

## TODOs

1. review Chaggai's truth table and the tools he developed
1. implement the above procedures using `bcftools` or the `vcf` python module.
